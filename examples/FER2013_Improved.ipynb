{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FER2013 Emotion Recognition - Advanced Implementation\n",
    "\n",
    "This notebook implements a state-of-the-art emotion recognition system using the FER2013 dataset.\n",
    "\n",
    "## Improvements:\n",
    "- Comprehensive data exploration and visualization\n",
    "- Advanced data augmentation techniques\n",
    "- Multiple model architectures (EfficientNet)\n",
    "- Better preprocessing and normalization\n",
    "- Ensemble methods\n",
    "- Real-time emotion detection interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GlobalAveragePooling2D, Input, BatchNormalization,\n",
    "    Conv2D, MaxPooling2D, Flatten, Activation, Add, AveragePooling2D,\n",
    "    LayerNormalization, SeparableConv2D  # Added SeparableConv2D for efficiency\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \n",
    "    TensorBoard, LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "\n",
    "# --- FIXED Configuration for FER2013 ---\n",
    "IMG_WIDTH, IMG_HEIGHT = 48, 48\n",
    "BATCH_SIZE = 64  # Increased for M3 Pro efficiency\n",
    "COLOR_MODE = 'grayscale'  # FIXED: FER2013 is grayscale\n",
    "NUM_CLASSES = 7\n",
    "EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Dataset paths\n",
    "BASE_DATASET_DIR = os.path.expanduser(\"~/Python/archive\")\n",
    "TRAIN_DIR = os.path.join(BASE_DATASET_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(BASE_DATASET_DIR, \"test\")\n",
    "\n",
    "# --- Mac M3 Pro Optimization ---\n",
    "# Disable mixed precision for stability on Apple Silicon\n",
    "print(\"\\nConfiguring for Apple Silicon M3 Pro...\")\n",
    "\n",
    "# Set memory growth for Apple Silicon\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"Metal GPU detected\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Verify dataset directories\n",
    "if os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR):\n",
    "    print(f\"\\nDataset directories found:\")\n",
    "    print(f\"  Training: {TRAIN_DIR}\")\n",
    "    print(f\"  Testing: {TEST_DIR}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset directories not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Exploration and Visualization\n",
    "\n",
    "print(\"=== Data Exploration ===\")\n",
    "\n",
    "# Function to count images per class\n",
    "def count_images_per_class(directory):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            class_counts[class_name] = count\n",
    "    return class_counts\n",
    "\n",
    "# Count images\n",
    "train_counts = count_images_per_class(TRAIN_DIR)\n",
    "test_counts = count_images_per_class(TEST_DIR)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training data distribution\n",
    "classes = list(train_counts.keys())\n",
    "train_values = [train_counts[c] for c in classes]\n",
    "test_values = [test_counts[c] for c in classes]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, train_values, width, label='Train', alpha=0.8)\n",
    "ax1.bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
    "ax1.set_xlabel('Emotion Class')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.set_title('Distribution of Images Across Classes')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(classes, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pie chart for training data\n",
    "ax2.pie(train_values, labels=classes, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Training Data Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total training images: {sum(train_values)}\")\n",
    "print(f\"Total test images: {sum(test_values)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls in classes:\n",
    "    print(f\"  {cls}: Train={train_counts[cls]}, Test={test_counts[cls]}\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "max_class = max(train_values)\n",
    "min_class = min(train_values)\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, emotion in enumerate(classes[:7]):\n",
    "    emotion_path = os.path.join(TRAIN_DIR, emotion)\n",
    "    sample_image = os.listdir(emotion_path)[0]\n",
    "    img_path = os.path.join(emotion_path, sample_image)\n",
    "    \n",
    "    img = load_img(img_path, color_mode='grayscale')\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    axes[idx].imshow(img_array.squeeze(), cmap='gray')\n",
    "    axes[idx].set_title(f'{emotion.capitalize()}\\n({train_counts[emotion]} images)')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Hide the last subplot if we have 7 emotions\n",
    "if len(classes) == 7:\n",
    "    axes[7].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Each Emotion Class', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Advanced Data Preprocessing and Augmentation\n",
    "\n",
    "print(\"=== Fixed Data Preprocessing for FER2013 ===\")\n",
    "\n",
    "# Proper preprocessing for FER2013\n",
    "def preprocess_input_fer2013(x):\n",
    "    \"\"\"Preprocessing optimized for FER2013 grayscale images\"\"\"\n",
    "    # Simple normalization works best for FER2013\n",
    "    x = x / 255.0\n",
    "    return x\n",
    "\n",
    "# Optimized augmentation for facial expressions\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_fer2013,\n",
    "    rotation_range=10,  # Reduced - faces don't rotate much\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,  # Reduced shear\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n",
    "# Only preprocessing for validation/test\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_fer2013\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "print(\"\\nCreating data generators...\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    color_mode=COLOR_MODE,  # Now grayscale\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    color_mode=COLOR_MODE,  # Now grayscale\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    color_mode=COLOR_MODE,  # Now grayscale\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate class weights\n",
    "class_labels = train_generator.classes\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(class_labels),\n",
    "    y=class_labels\n",
    ")\n",
    "CLASS_WEIGHTS = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(f\"\\nData generators created:\")\n",
    "print(f\"  Training samples: {train_generator.samples}\")\n",
    "print(f\"  Validation samples: {validation_generator.samples}\")\n",
    "print(f\"  Test samples: {test_generator.samples}\")\n",
    "print(f\"\\nClass weights: {CLASS_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Custom CNN Architecture Optimized for FER2013\n",
    "\n",
    "print(\"=== Building Custom CNN for FER2013 ===\")\n",
    "\n",
    "def create_fer2013_cnn(input_shape=(IMG_HEIGHT, IMG_WIDTH, 1), \n",
    "                       num_classes=NUM_CLASSES,\n",
    "                       l2_reg=0.0001):\n",
    "    \"\"\"\n",
    "    Custom CNN optimized for FER2013 48x48 grayscale images\n",
    "    Efficient architecture for M3 Pro Mac\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Block 1 - Initial feature extraction\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(l2_reg), \n",
    "               input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2 - Deeper features\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3 - Complex patterns\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 4 - High-level features (using SeparableConv2D for efficiency)\n",
    "        SeparableConv2D(512, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        GlobalAveragePooling2D(),  # More efficient than Flatten\n",
    "        \n",
    "        # Classification head\n",
    "        Dense(256, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_mini_xception()  # or create_fer2013_cnn() for standard CNN\n",
    "\n",
    "# Compile with optimized settings for M3 Pro\n",
    "optimizer = Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    model, \n",
    "    to_file='fer2013_model_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Optimized Training for M3 Pro\n",
    "\n",
    "print(\"=== Optimized Training Setup ===\")\n",
    "\n",
    "# Cosine annealing with warm restarts\n",
    "def cosine_annealing_with_warmup(epoch, lr):\n",
    "    \"\"\"Cosine annealing with initial warmup\"\"\"\n",
    "    warmup_epochs = 5\n",
    "    max_epochs = 50\n",
    "    initial_lr = 0.001\n",
    "    min_lr = 1e-6\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)\n",
    "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "# Optimized callbacks for FER2013\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_fer2013_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    LearningRateScheduler(cosine_annealing_with_warmup, verbose=0)\n",
    "]\n",
    "\n",
    "# Training configuration optimized for M3 Pro\n",
    "EPOCHS = 50  # Reduced from 80 total\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Initial learning rate: {optimizer.learning_rate.numpy()}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    verbose=1,\n",
    "    workers=4,  # Optimize for M3 Pro\n",
    "    use_multiprocessing=False  # Better stability on Mac\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fine-tuning Phase\n",
    "\n",
    "print(\"\\n=== Phase 2: Fine-tuning ===\")\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers) - 20\n",
    "\n",
    "# Freeze all the layers before fine_tune_at\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Unfreezing top {len(base_model.layers) - fine_tune_at} layers of base model\")\n",
    "print(f\"Total trainable layers: {len([l for l in model.layers if l.trainable])}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "fine_tune_lr = 1e-5\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=fine_tune_lr),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=INITIAL_EPOCHS + FINE_TUNE_EPOCHS,\n",
    "    initial_epoch=len(initial_history['loss']),\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combine histories\n",
    "history = {}\n",
    "for key in initial_history.keys():\n",
    "    history[key] = initial_history[key] + history_fine.history[key]\n",
    "\n",
    "print(\"\\nFine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Comprehensive Model Evaluation\n",
    "\n",
    "print(\"=== Model Evaluation ===\")\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model('best_emotion_model.keras')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_precision, test_recall = best_model.evaluate(\n",
    "    test_generator,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = best_model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Ensure alignment\n",
    "y_true = y_true[:len(y_pred_classes)]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_true, y_pred_classes, \n",
    "                          target_names=EMOTION_LABELS,\n",
    "                          digits=3))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=EMOTION_LABELS,\n",
    "            yticklabels=EMOTION_LABELS,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Emotion Recognition Model', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(EMOTION_LABELS, per_class_accuracy)\n",
    "plt.title('Per-Class Accuracy', fontsize=16)\n",
    "plt.xlabel('Emotion', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, per_class_accuracy):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training History Visualization\n",
    "\n",
    "print(\"=== Training History Visualization ===\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 0].plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 0].axvline(x=INITIAL_EPOCHS, color='red', linestyle='--', label='Start Fine-tuning')\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history['loss'], label='Training Loss')\n",
    "axes[0, 1].plot(history['val_loss'], label='Validation Loss')\n",
    "axes[0, 1].axvline(x=INITIAL_EPOCHS, color='red', linestyle='--', label='Start Fine-tuning')\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "if 'lr' in history:\n",
    "    axes[1, 0].plot(history['lr'], label='Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision and Recall\n",
    "if 'precision' in history:\n",
    "    axes[1, 1].plot(history['precision'], label='Training Precision')\n",
    "    axes[1, 1].plot(history['val_precision'], label='Validation Precision')\n",
    "    axes[1, 1].plot(history['recall'], label='Training Recall', linestyle='--')\n",
    "    axes[1, 1].plot(history['val_recall'], label='Validation Recall', linestyle='--')\n",
    "    axes[1, 1].set_title('Precision and Recall', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"  Best Validation Accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"  Best Validation Loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Final Training Accuracy: {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Accuracy: {history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Fixed Real-time Emotion Detection\n",
    "\n",
    "print(\"=== Emotion Detection Interface ===\")\n",
    "\n",
    "def preprocess_image(image_path, target_size=(48, 48)):\n",
    "    \"\"\"\n",
    "    Preprocess a single image for prediction\n",
    "    \"\"\"\n",
    "    # Load as grayscale\n",
    "    img = load_img(image_path, target_size=target_size, color_mode='grayscale')\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    # Normalize\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def predict_emotion(model, image_path, show_probabilities=True):\n",
    "    \"\"\"\n",
    "    Predict emotion from an image file\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(processed_image, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    # Get emotion label\n",
    "    predicted_emotion = EMOTION_LABELS[predicted_class]\n",
    "    \n",
    "    # Display results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Show image\n",
    "    img = load_img(image_path, color_mode='grayscale')\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.set_title(f'Predicted: {predicted_emotion} ({confidence*100:.1f}%)', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show probability distribution\n",
    "    if show_probabilities:\n",
    "        bars = ax2.bar(EMOTION_LABELS, predictions[0])\n",
    "        ax2.set_title('Emotion Probabilities', fontsize=14)\n",
    "        ax2.set_xlabel('Emotion')\n",
    "        ax2.set_ylabel('Probability')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Highlight predicted emotion\n",
    "        bars[predicted_class].set_color('red')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (emotion, prob) in enumerate(zip(EMOTION_LABELS, predictions[0])):\n",
    "            ax2.text(i, prob + 0.01, f'{prob:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_emotion, confidence, predictions[0]\n",
    "\n",
    "# Save the final model\n",
    "best_model = tf.keras.models.load_model('best_fer2013_model.keras')\n",
    "best_model.save('fer2013_emotion_detector_final.keras')\n",
    "print(\"\\nFinal model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Export and Deployment Preparation\n",
    "\n",
    "print(\"=== Model Export for Deployment ===\")\n",
    "\n",
    "# Save model in multiple formats\n",
    "# 1. SavedModel format (for TensorFlow Serving)\n",
    "best_model.save('emotion_detector_savedmodel', save_format='tf')\n",
    "print(\"Model saved in SavedModel format for TensorFlow Serving\")\n",
    "\n",
    "# 2. TFLite conversion for mobile deployment\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('emotion_detector.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Model converted to TFLite format for mobile deployment\")\n",
    "\n",
    "# 3. Save model configuration and weights separately\n",
    "model_json = best_model.to_json()\n",
    "with open('emotion_detector_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "best_model.save_weights('emotion_detector_weights.h5')\n",
    "print(\"Model architecture and weights saved separately\")\n",
    "\n",
    "# Create a simple inference script\n",
    "inference_script = '''import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('fer2013_emotion_detector_final.keras')\n",
    "\n",
    "# Emotion labels\n",
    "EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "def predict_emotion(image_path):\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = np.array(img)\n",
    "    img_array = (img_array - 127.5) / 127.5\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    emotion_idx = np.argmax(predictions[0])\n",
    "    emotion = EMOTION_LABELS[emotion_idx]\n",
    "    confidence = predictions[0][emotion_idx]\n",
    "    \n",
    "    return emotion, confidence\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) > 1:\n",
    "        emotion, confidence = predict_emotion(sys.argv[1])\n",
    "        print(f\"Emotion: {emotion} (Confidence: {confidence:.2%})\")\n",
    "    else:\n",
    "        print(\"Usage: python predict_emotion.py <image_path>\")\n",
    "'''\n",
    "\n",
    "with open('predict_emotion.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "print(\"\\nInference script created: predict_emotion.py\")\n",
    "\n",
    "# Model summary for deployment\n",
    "print(\"\\n=== Deployment Summary ===\")\n",
    "print(f\"Model input shape: {best_model.input_shape}\")\n",
    "print(f\"Model output shape: {best_model.output_shape}\")\n",
    "print(f\"Total parameters: {best_model.count_params():,}\")\n",
    "print(f\"Model size (Keras): {os.path.getsize('fer2013_emotion_detector_final.keras') / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Model size (TFLite): {os.path.getsize('emotion_detector.tflite') / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"\\n✅ Model is ready for deployment!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use 'fer2013_emotion_detector_final.keras' for Python applications\")\n",
    "print(\"2. Use 'emotion_detector.tflite' for mobile applications\")\n",
    "print(\"3. Use 'emotion_detector_savedmodel' for TensorFlow Serving\")\n",
    "print(\"4. Run 'python predict_emotion.py <image_path>' for quick predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
